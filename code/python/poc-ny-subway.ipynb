{"cells":[{"cell_type":"code","source":["from pyspark import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.mllib.stat import Statistics\nfrom pyspark.ml.regression import LinearRegression\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["## Exploratory data analysis (EDA) - Part 1"],"metadata":{}},{"cell_type":"code","source":["# Carrega dataset do desafio (basta descompactar todos na mesma pasta)\n#display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))\ndf_desafio = spark.read.format(\"csv\").options(header='true').load(\"/FileStore/tables/*.csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Elimina na's e duplicados do df\ndf_desafio_v2 = df_desafio.dropna(how='any').dropDuplicates()\n\n# Ajusta tipo de colunas\ndf_desafio_v2 = df_desafio_v2.selectExpr(\n  'cast(time as timestamp) time',\n  'ca',\n  'unit',\n  'scp',\n  'station',\n  'linename',\n  'division',\n  'desc',\n  'cast(entries as int) entries',\n  'cast(exits as int) exits'\n)\n\n# Features para visão temporal\ndf_desafio_v2 = df_desafio_v2.withColumn(\n  \"dt_year\",\n  year(col(\"time\"))\n).withColumn(\n  \"dt_month\",\n  month(col(\"time\"))\n).withColumn(\n  \"dt_day\",\n  dayofmonth(col(\"time\"))\n).withColumn(\n  \"dt_dayofy\",\n  dayofyear(col(\"time\"))  \n).withColumn(\n  \"dt_hour\",\n  hour(col(\"time\"))\n).withColumn(\n  \"dt_min\",\n  minute(col(\"time\"))\n).withColumn(\n  \"dt_week_no\",\n  weekofyear(col(\"time\"))\n).withColumn(\n  \"dt_int\",\n  unix_timestamp(col(\"time\"))\n).withColumn(\n  \"dt_month_year\",\n  date_format(col(\"time\"), \"Y-MM\")\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Intervalos para buckets\nsplits = [-30000000, -20000000, -10000000, 0.0, 10000000, 20000000, 30000000]\n\n# ===> Bucket: 'entries'\n# dataFrame = df_desafio_v2.select(col('entries'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"entries\", outputCol=\"bucketedFeatures\")\n# bucketedData_entries = bucketizer.transform(dataFrame)\n# sorted(bucketedData_entries.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=15721),\n#  Row(bucketedFeatures=1.0, count=4028),\n#  Row(bucketedFeatures=2.0, count=496),\n#  Row(bucketedFeatures=4.0, count=71457425), ==> Begin\n#  Row(bucketedFeatures=5.0, count=5052549),  <== End\n#  Row(bucketedFeatures=6.0, count=278162),\n#  Row(bucketedFeatures=7.0, count=2237294)]\n\n# ===> Bucket: 'exits'\n# dataFrame = df_desafio_v2.select(col('exits'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"exits\", outputCol=\"bucketedFeatures\")\n# bucketedData_exits = bucketizer.transform(dataFrame)\n# sorted(bucketedData_exits.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=30576),\n#  Row(bucketedFeatures=2.0, count=576),\n#  Row(bucketedFeatures=3.0, count=1),\n#  Row(bucketedFeatures=4.0, count=73751241), ==> Begin\n#  Row(bucketedFeatures=5.0, count=3698911),  <== End\n#  Row(bucketedFeatures=6.0, count=413740),\n#  Row(bucketedFeatures=7.0, count=1150630)]\n\n# Parâmetros para filtros de outliers\noutlier_begin = 0\noutlier_end = 20000000\n\ndf_desafio_v2 = df_desafio_v2.where((col('entries')>=outlier_begin) & (col('entries')<=outlier_end) & (col('exits')>=outlier_begin) & (col('exits')<=outlier_end))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Check: 79.609.191 / 79.130.015 / 79.045.675 / 75.923.980\ncount_desafio = df_desafio.count()\ncount_desafio_na = df_desafio.dropna(how='any').count()\ncount_desafio_final = df_desafio.dropna(how='any').dropDuplicates().count()\ncount_desafio_outliers = df_desafio_v2.count()\n\ndf_amostras = sc.parallelize([\n  ('antes',count_desafio,0,0,0,0),\n  ('depois',0,count_desafio_final,count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','TOTAL','UNICO','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df_amostras_v2 = sc.parallelize([\n  ('',count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras_v2)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Método para variáveis categóricas (dummys) ~14.59 minutes\nlista_idx = ['ca', 'unit', 'scp', 'station', 'linename', 'division', 'desc']\nindexers = [StringIndexer(inputCol=column,outputCol=column+\"_idx\").fit(df_desafio_v2) for column in lista_idx]\npipeline = Pipeline(stages=indexers)\ndf_desafio_v2 = pipeline.fit(df_desafio_v2).transform(df_desafio_v2)\n#display(df_desafio_v2)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df_desafio_v2.write.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio_num.write.parquet(\"/FileStore/tables/df_desafio_ml.parquet\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Exploratory data analysis (EDA) - Part 2"],"metadata":{}},{"cell_type":"code","source":["# Carrega Parquet\ndf_desafio_pqt = spark.read.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio = df_desafio_pqt.select(\n  'entries','exits','dt_year','dt_month','dt_day','dt_dayofy',\n  'dt_hour','dt_min','dt_week_no','dt_int','ca_idx','unit_idx',\n  'scp_idx','station_idx','linename_idx','division_idx','desc_idx'\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["col_names = df_desafio.columns\nfeatures = df_desafio.rdd.map(lambda row: row[0:])\ncorr_mat = Statistics.corr(features, method=\"pearson\")\ncorr_df = pd.DataFrame(corr_mat)\ncorr_df.index, corr_df.columns = col_names, col_names\ncorr_df\n#Falta gráfico"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df_desafio_num.describe().toPandas().transpose()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df_desafio_num.dtypes"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["sampled_data = df_desafio_num.select(df_desafio_num.columns).sample(False, 0.8).toPandas()\naxs = pd.scatter_matrix(sampled_data, figsize=(10, 10))\nn = len(sampled_data.columns)\n\n# for i in range(n):\n#     v = axs[i, 0]\n#     v.yaxis.label.set_rotation(0)\n#     v.yaxis.label.set_ha('right')\n#     v.set_yticks(())\n#     h = axs[n-1, i]\n#     h.xaxis.label.set_rotation(90)\n#     h.set_xticks(())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Exploratory data analysis (EDA) - Part 3"],"metadata":{}},{"cell_type":"code","source":["#df_graficos = df_desafio_v2.filter(col('dt_year')=='2017')\ndf_graficos = df_desafio_v2.groupBy(\n  'dt_month_year',\n  'dt_year',\n  'dt_month',\n  'dt_day',\n  'dt_hour'\n).agg(\n  sum('entries'),\n  sum('exits')\n).orderBy(\n  \"dt_month_year\"\n).toPandas()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#display(df_graficos)"],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"poc-ny-subway","notebookId":1932292055738400},"nbformat":4,"nbformat_minor":0}
