{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\nfrom pyspark import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import StringIndexer, Bucketizer, VectorAssembler, StandardScaler\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.mllib.stat import Statistics\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nimport tensorflow as tf"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["#### Exploratory data analysis (EDA)"],"metadata":{}},{"cell_type":"code","source":["# Carrega dataset do desafio (basta descompactar todos na mesma pasta)\n#display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))\ndf_desafio = spark.read.format(\"csv\").options(header='true').load(\"/FileStore/tables/*.csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Elimina na's e duplicados do df\ndf_desafio_v2 = df_desafio.dropna(how='any').dropDuplicates()\n\n# Ajusta tipo de colunas\ndf_desafio_v2 = df_desafio_v2.selectExpr(\n  'cast(time as timestamp) time',\n  'ca',\n  'unit',\n  'scp',\n  'station',\n  'linename',\n  'division',\n  'desc',\n  'cast(entries as int) entries',\n  'cast(exits as int) exits'\n)\n\n# Features para visão temporal\ndf_desafio_v2 = df_desafio_v2.withColumn(\n  \"dt_year\",\n  year(col(\"time\"))\n).withColumn(\n  \"dt_month\",\n  month(col(\"time\"))\n).withColumn(\n  \"dt_day\",\n  dayofmonth(col(\"time\"))\n).withColumn(\n  \"dt_dayofy\",\n  dayofyear(col(\"time\"))  \n).withColumn(\n  \"dt_hour\",\n  hour(col(\"time\"))\n).withColumn(\n  \"dt_min\",\n  minute(col(\"time\"))\n).withColumn(\n  \"dt_week_no\",\n  weekofyear(col(\"time\"))\n).withColumn(\n  \"dt_int\",\n  unix_timestamp(col(\"time\"))\n).withColumn(\n  \"dt_month_year\",\n  date_format(col(\"time\"), \"Y-MM\")\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Intervalos para buckets\nsplits = [-30000000, -20000000, -10000000, 0.0, 10000000, 20000000, 30000000]\n\n# ===> Bucket: 'entries'\n# dataFrame = df_desafio_v2.select(col('entries'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"entries\", outputCol=\"bucketedFeatures\")\n# bucketedData_entries = bucketizer.transform(dataFrame)\n# sorted(bucketedData_entries.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=15721),\n#  Row(bucketedFeatures=1.0, count=4028),\n#  Row(bucketedFeatures=2.0, count=496),\n#  Row(bucketedFeatures=4.0, count=71457425), ==> Begin\n#  Row(bucketedFeatures=5.0, count=5052549),  <== End\n#  Row(bucketedFeatures=6.0, count=278162),\n#  Row(bucketedFeatures=7.0, count=2237294)]\n\n# ===> Bucket: 'exits'\n# dataFrame = df_desafio_v2.select(col('exits'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"exits\", outputCol=\"bucketedFeatures\")\n# bucketedData_exits = bucketizer.transform(dataFrame)\n# sorted(bucketedData_exits.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=30576),\n#  Row(bucketedFeatures=2.0, count=576),\n#  Row(bucketedFeatures=3.0, count=1),\n#  Row(bucketedFeatures=4.0, count=73751241), ==> Begin\n#  Row(bucketedFeatures=5.0, count=3698911),  <== End\n#  Row(bucketedFeatures=6.0, count=413740),\n#  Row(bucketedFeatures=7.0, count=1150630)]\n\n# Parâmetros para filtros de outliers\noutlier_begin = 0\noutlier_end = 20000000\n\ndf_desafio_v2 = df_desafio_v2.where((col('entries')>=outlier_begin) & (col('entries')<=outlier_end) & (col('exits')>=outlier_begin) & (col('exits')<=outlier_end))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Check: 79.609.191 / 79.130.015 / 79.045.675 / 75.923.980\ncount_desafio = df_desafio.count()\ncount_desafio_na = df_desafio.dropna(how='any').count()\ncount_desafio_final = df_desafio.dropna(how='any').dropDuplicates().count()\ncount_desafio_outliers = df_desafio_v2.count()\n\ndf_amostras = sc.parallelize([\n  ('antes',count_desafio,0,0,0,0),\n  ('depois',0,count_desafio_final,count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','TOTAL','UNICO','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df_amostras_v2 = sc.parallelize([\n  ('',count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras_v2)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Método para variáveis categóricas (dummys) ~14.59 minutes\nlista_idx = ['ca', 'unit', 'scp', 'station', 'linename', 'division', 'desc']\nindexers = [StringIndexer(inputCol=column,outputCol=column+\"_idx\").fit(df_desafio_v2) for column in lista_idx]\npipeline = Pipeline(stages=indexers)\ndf_desafio_v2 = pipeline.fit(df_desafio_v2).transform(df_desafio_v2)\n#display(df_desafio_v2)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df_desafio_v2.write.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio_num.write.parquet(\"/FileStore/tables/df_desafio_ml.parquet\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#df_graficos = df_desafio_v2.filter(col('dt_year')=='2017')\ndf_graficos = df_desafio_v2.groupBy(\n  'dt_month_year',\n  'dt_year',\n  'dt_month',\n  'dt_day',\n  'dt_hour'\n).agg(\n  sum('entries'),\n  sum('exits')\n).orderBy(\n  \"dt_month_year\"\n).toPandas()\n\n#df_desafio_num.describe().toPandas().transpose()\n#df_desafio_num.dtypes\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Carga de Parquet para modelos"],"metadata":{}},{"cell_type":"code","source":["# Carrega Parquet\ndf_desafio_pqt = spark.read.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio_num = df_desafio_pqt.select(\n  'entries','exits','dt_year','dt_month','dt_day','dt_dayofy',\n  'dt_hour','dt_min','dt_week_no','dt_int','ca_idx','unit_idx',\n  'scp_idx','station_idx','linename_idx','division_idx','desc_idx'\n)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#### Correlação entre features"],"metadata":{}},{"cell_type":"code","source":["col_names = df_desafio.columns\nfeatures = df_desafio.rdd.map(lambda row: row[0:])\ncorr_mat = Statistics.corr(features, method=\"pearson\")\ncorr_df = pd.DataFrame(corr_mat)\ncorr_df.index, corr_df.columns = col_names, col_names\nspark.createDataFrame(corr_df).write.parquet(\"/FileStore/tables/corr_df.parquet\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### Modelo de Machine Learning - Regressão Linear"],"metadata":{}},{"cell_type":"code","source":["# Verorização de features\ncolunas = ['exits', 'dt_year', 'dt_month', 'dt_day', 'dt_dayofy', 'dt_hour', 'dt_min', 'dt_week_no', 'dt_int', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx', 'linename_idx', 'division_idx', 'desc_idx']\nvectorAssembler = VectorAssembler(inputCols = colunas, outputCol = 'features')\nvdf_desafio_num = vectorAssembler.transform(df_desafio_num)\nvdf_desafio_num.take(1)\n\n# Seleciona features de interesse\nvdf_desafio_num = vdf_desafio_num.select(['features', 'entries'])\nvdf_desafio_num.show(3)\n\n# Split para dados de treino/teste\nsplits = vdf_desafio_num.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Treinando os dados de treino para predizer os dados de teste\nlr = LinearRegression(featuresCol = 'features', labelCol='entries', maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlr_model = lr.fit(train_df)\n\n#Salva modelo treinado\nlr_model.save(\"/FileStore/tables/lr_model\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Resumo do treinamento\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"Intercept: \" + str(lr_model.intercept))\n\n# Resumo do modelo treinado\ntrainingSummary = lr_model.summary\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2: %f\" % trainingSummary.r2)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Descritiva da variável de interesse treinada\ntrain_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Predição dos dados de teste vs Real\nlr_predictions = lr_model.transform(test_df)\nlr_predictions.select(\"prediction\",\"entries\",\"features\").show(5)\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"entries\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n\n# Resultado RMSE do modelo de teste\ntest_result = lr_model.evaluate(test_df)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Histograma para residuals\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n#display(trainingSummary.residuals)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#### Modelo de Deep Learning - TensorFlow"],"metadata":{}},{"cell_type":"code","source":["# Etapa 01 - Transformações: VectorAssembler\n\n# Variáveis para modelo\nfeatures = ['entries', 'exits', 'dt_year', 'dt_month', 'dt_day', 'dt_dayofy', 'dt_hour', 'dt_min', 'dt_week_no', 'dt_int', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx', 'linename_idx', 'division_idx', 'desc_idx']\nvar_predict = 'entries'\nfeatures.remove(var_predict)\n\n# Vetorização de features\nvectorAssembler = VectorAssembler(inputCols = features, outputCol = 'features')\ndf_desafio = vectorAssembler.transform(df_desafio_num)\n\n# Seleciona features + interesse (y)\ndf_desafio = df_desafio.select(['features', var_predict])\n\n# Divisão de dados para treino e teste\nsplits = df_desafio.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Dados para treino e teste\nx_train = np.array(train_df.limit(100).select('features').collect())\ny_train = np.array(train_df.limit(100).select(var_predict).collect())\nx_test = np.array(test_df.limit(100).select('features').collect())\ny_test = np.array(test_df.limit(100).select(var_predict).collect())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["#Config do Modelo\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(30000000, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5)\n# model.evaluate(x_test, y_test)"],"metadata":{},"outputs":[],"execution_count":25}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"poc-ny-subway","notebookId":1932292055738400},"nbformat":4,"nbformat_minor":0}
