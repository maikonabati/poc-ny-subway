{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\nfrom pyspark import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, Bucketizer, VectorAssembler, StandardScaler\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.mllib.stat import Statistics\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nfrom elephas.ml_model import ElephasEstimator\nfrom elephas import optimizers as elephas_optimizers"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["#### Exploratory data analysis (EDA)"],"metadata":{}},{"cell_type":"code","source":["# Carrega dataset do desafio (basta descompactar todos na mesma pasta)\n#display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))\ndf_desafio = spark.read.format(\"csv\").options(header='true').load(\"/FileStore/tables/*.csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Elimina na's e duplicados do df\ndf_desafio_v2 = df_desafio.dropna(how='any').dropDuplicates()\n\n# Ajusta tipo de colunas\ndf_desafio_v2 = df_desafio_v2.selectExpr(\n  'cast(time as timestamp) time',\n  'ca',\n  'unit',\n  'scp',\n  'station',\n  'linename',\n  'division',\n  'desc',\n  'cast(entries as int) entries',\n  'cast(exits as int) exits'\n)\n\n# Features para visão temporal\ndf_desafio_v2 = df_desafio_v2.withColumn(\n  \"dt_year\",\n  year(col(\"time\"))\n).withColumn(\n  \"dt_month\",\n  month(col(\"time\"))\n).withColumn(\n  \"dt_day\",\n  dayofmonth(col(\"time\"))\n).withColumn(\n  \"dt_dayofy\",\n  dayofyear(col(\"time\"))  \n).withColumn(\n  \"dt_hour\",\n  hour(col(\"time\"))\n).withColumn(\n  \"dt_min\",\n  minute(col(\"time\"))\n).withColumn(\n  \"dt_week_no\",\n  weekofyear(col(\"time\"))\n).withColumn(\n  \"dt_int\",\n  unix_timestamp(col(\"time\"))\n).withColumn(\n  \"dt_month_year\",\n  date_format(col(\"time\"), \"Y-MM\")\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Intervalos para buckets\nsplits = [-30000000, -20000000, -10000000, 0.0, 10000000, 20000000, 30000000]\n\n# ===> Bucket: 'entries'\n# dataFrame = df_desafio_v2.select(col('entries'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"entries\", outputCol=\"bucketedFeatures\")\n# bucketedData_entries = bucketizer.transform(dataFrame)\n# sorted(bucketedData_entries.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=15721),\n#  Row(bucketedFeatures=1.0, count=4028),\n#  Row(bucketedFeatures=2.0, count=496),\n#  Row(bucketedFeatures=4.0, count=71457425), ==> Begin\n#  Row(bucketedFeatures=5.0, count=5052549),  <== End\n#  Row(bucketedFeatures=6.0, count=278162),\n#  Row(bucketedFeatures=7.0, count=2237294)]\n\n# ===> Bucket: 'exits'\n# dataFrame = df_desafio_v2.select(col('exits'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"exits\", outputCol=\"bucketedFeatures\")\n# bucketedData_exits = bucketizer.transform(dataFrame)\n# sorted(bucketedData_exits.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=30576),\n#  Row(bucketedFeatures=2.0, count=576),\n#  Row(bucketedFeatures=3.0, count=1),\n#  Row(bucketedFeatures=4.0, count=73751241), ==> Begin\n#  Row(bucketedFeatures=5.0, count=3698911),  <== End\n#  Row(bucketedFeatures=6.0, count=413740),\n#  Row(bucketedFeatures=7.0, count=1150630)]\n\n# Parâmetros para filtros de outliers\noutlier_begin = 0\noutlier_end = 20000000\n\ndf_desafio_v2 = df_desafio_v2.where((col('entries')>=outlier_begin) & (col('entries')<=outlier_end) & (col('exits')>=outlier_begin) & (col('exits')<=outlier_end))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Check: 79.609.191 / 79.130.015 / 79.045.675 / 75.923.980\ncount_desafio = df_desafio.count()\ncount_desafio_na = df_desafio.dropna(how='any').count()\ncount_desafio_final = df_desafio.dropna(how='any').dropDuplicates().count()\ncount_desafio_outliers = df_desafio_v2.count()\n\ndf_amostras = sc.parallelize([\n  ('antes',count_desafio,0,0,0,0),\n  ('depois',0,count_desafio_final,count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','TOTAL','UNICO','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df_amostras_v2 = sc.parallelize([\n  ('',count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras_v2)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Método para variáveis categóricas (dummys) ~14.59 minutes\nlista_idx = ['ca', 'unit', 'scp', 'station', 'linename', 'division', 'desc']\nindexers = [StringIndexer(inputCol=column,outputCol=column+\"_idx\").fit(df_desafio_v2) for column in lista_idx]\npipeline = Pipeline(stages=indexers)\ndf_desafio_v2 = pipeline.fit(df_desafio_v2).transform(df_desafio_v2)\n#display(df_desafio_v2)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df_desafio_v2.write.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio_num.write.parquet(\"/FileStore/tables/df_desafio_ml.parquet\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df_desafio = spark.read.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio.registerTempTable(\"df_desafio\")\n\ndf_desafio_graf = df_desafio.groupBy(\n  'dt_month_year',\n  'dt_year',\n  'dt_month',\n  'dt_day',\n  'dt_hour'\n).agg(\n  sum('entries'),\n  sum('exits')\n).orderBy(\n  \"dt_month_year\"\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Histogramas com Pandas\n#df = df_desafio_graf.toPandas()\ndf.hist('dt_month')\ndisplay()\n# df_desafio_graf.filter(col('dt_year')=='2011').count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Descritiva das variáveis de interesse\nvar_interesse = 'entries' #exits\ndf = sqlContext.sql(\"select 'entries' as var, min(entries) as minimo,percentile_approx(entries, 0.5) as mediana, int(avg(entries)) as media, max(entries) as maximo from df_desafio union select 'exits' as var, min(exits) as minimo,percentile_approx(exits, 0.5) as mediana, int(avg(exits)) as media, max(exits) as maximo from df_desafio\")\ndf.show(10)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Relação direita entre variáveis\nvar_interesse = 'exits'\ncolunas = ['dt_year', 'dt_month', 'dt_day', 'dt_hour','entries', 'exits']\n\nsns.pairplot(df,x_vars='entries',y_vars=var_interesse)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Base para Regressão Simples\ndf_desafio_rl = df_desafio.groupBy(  \n  'dt_year',\n  'dt_month',\n  'ca_idx',\n  'unit_idx',\n  'scp_idx',\n  'station_idx',\n  'linename_idx',\n  'division_idx',\n  'desc_idx'\n).agg(\n  sum('entries'),\n  sum('exits')\n)\n#df_desafio_rl.count()\ndf_rl = df_desafio_rl.toPandas()\ndf_rl.columns = ['dt_year', 'dt_month', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx','linename_idx', 'division_idx', 'desc_idx', 'entries','exits']"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Regressão Simples \nvar_interesse = 'exits'\ncolunas = df_rl.columns\n\nfig, ax = plt.subplots(3, 6, figsize = (25, 15))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_rl.columns):\n        sns.regplot(x=colunas[i],y=var_interesse, data=df_rl[:100000], ax=ax)        \ndisplay()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Gráfico de barras\ndisplay(df_desafio.filter(col('dt_year')=='2016').groupBy(  \n  'dt_month_year'  \n).agg(\n  sum('entries'),\n  sum('exits')\n).orderBy('dt_month_year')\n)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Base para Boxplot\ndf_desafio_box = df_desafio.filter(col('dt_year') == '2016').groupBy(\n  'dt_month_year',\n  'dt_year',\n  'dt_month',\n  'dt_day',\n  'dt_hour'\n).agg(\n  sum('entries'),\n  sum('exits')\n).orderBy('dt_month_year')\n#df_desafio_rl.count()\ndf_box = df_desafio_box.toPandas()\ndf_box.columns = ['dt_month_year', 'dt_year', 'dt_month', 'dt_day', 'dt_hour', 'entries', 'exits']"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Gráfico Boxplot\nplt.figure(figsize = (10, 6))\nax = sns.boxplot(x='dt_month_year', y='entries', data=df_box)\nplt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")\nplt.ylim(0, 20000000000)\nplt.xticks(rotation=45)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["#### Carga de Parquet para modelos"],"metadata":{}},{"cell_type":"code","source":["# Carrega Parquet\ndf_desafio_pqt = spark.read.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio_num = df_desafio_pqt.select(\n  'entries','exits','dt_year','dt_month','dt_day','dt_dayofy',\n  'dt_hour','dt_min','dt_week_no','dt_int','ca_idx','unit_idx',\n  'scp_idx','station_idx','linename_idx','division_idx','desc_idx'\n)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### Correlação entre features"],"metadata":{}},{"cell_type":"code","source":["# Gera matriz de correlação\ncol_names = df_desafio_num.columns\nfeatures = df_desafio_num.rdd.map(lambda row: row[0:])\ncorr_mat = Statistics.corr(features, method=\"pearson\")\ncorr_df = pd.DataFrame(corr_mat)\ncorr_df.index, corr_df.columns = col_names, col_names\nspark.createDataFrame(corr_df).write.parquet(\"/FileStore/tables/corr_df.parquet\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Matriz de correlação - Gráfio\n#corr_pq = spark.read.parquet(\"/FileStore/tables/corr_df.parquet\")\ncorr = corr_df\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(10, 220, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,annot=True, annot_kws={\"size\": 8}, \n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\ndisplay()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["#### Modelo de Machine Learning - Regressão Linear"],"metadata":{}},{"cell_type":"code","source":["# Vetorização de features\ncolunas = ['exits', 'dt_year', 'dt_month', 'dt_day', 'dt_dayofy', 'dt_hour', 'dt_min', 'dt_week_no', 'dt_int', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx', 'linename_idx', 'division_idx', 'desc_idx']\nvectorAssembler = VectorAssembler(inputCols = colunas, outputCol = 'features')\nvdf_desafio_num = vectorAssembler.transform(df_desafio_num)\nvdf_desafio_num.take(1)\n\n# Seleciona features de interesse\nvdf_desafio_num = vdf_desafio_num.select(['features', 'entries'])\nvdf_desafio_num.show(3)\n\n# Split para dados de treino/teste\nsplits = vdf_desafio_num.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Treinando o modelo com dados de treino para predizer os dados de teste\nlr = LinearRegression(featuresCol = 'features', labelCol='entries', maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlr_model = lr.fit(train_df)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#Salva modelo treinado\n#lr_model.save(\"/FileStore/tables/lr_model_entries\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Predição dos dados de teste vs Real\nlr_predictions = lr_model.transform(test_df)\nlr_predictions.select(\"prediction\",\"entries\",\"features\").show(5)\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"entries\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n\n# Resultado RMSE do modelo de teste\ntest_result = lr_model.evaluate(test_df)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Histograma para residuals\ndisplay(trainingSummary.residuals)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Preparação de dataset\n# Vetorização de features\ncolunas = ['exits', 'dt_year', 'dt_month', 'dt_day', 'dt_dayofy', 'dt_hour', 'dt_min', 'dt_week_no', 'dt_int', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx', 'linename_idx', 'division_idx', 'desc_idx']\nvectorAssembler = VectorAssembler(inputCols = colunas, outputCol = 'features')\nvdf_desafio_num = vectorAssembler.transform(df_desafio_num.filter(col('dt_year')=='2016'))\n\n# Seleciona features de interesse\nvdf_desafio_num = vdf_desafio_num.select(['features', 'entries'])\ntrain_df = vdf_desafio_num"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Gráfico 2016\nlr_predictions = lr_model.transform(train_df)\nlr_predictions.select(\"prediction\",\"entries\",\"features\").show(5)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["#lr_predictions.write.parquet(\"/FileStore/tables/lr_predictions_entries.parquet\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["lr_predictions_entries = spark.read.parquet(\"/FileStore/tables/lr_predictions_entries.parquet\")\nlr_predictions_exits = spark.read.parquet(\"/FileStore/tables/lr_predictions_exits.parquet\")\n\ndf_desafio_num = df_desafio_pqt.filter(col('dt_year')=='2016').select('dt_year','dt_month','dt_day','dt_dayofy','dt_hour','dt_min','dt_week_no','dt_int','ca_idx','unit_idx','scp_idx','station_idx','linename_idx','division_idx','desc_idx')"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["lr_predictions_entries = lr_predictions_entries.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\nlr_predictions_exits = lr_predictions_exits.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\ndf_desafio_num = df_desafio_num.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\nlr_predictions_entries.registerTempTable(\"lr_predictions_entries\")\nlr_predictions_exits.registerTempTable(\"lr_predictions_exits\")\ndf_desafio_num.registerTempTable(\"df_desafio_num\")\n\ndf_2016 = sqlContext.sql(\n  \"select \\\n    dt_year, dt_month, dt_day, dt_dayofy, dt_hour, dt_min, dt_week_no, \\\n    dt_int, ca_idx, unit_idx, scp_idx, station_idx, linename_idx, division_idx, desc_idx, \\\n    int(b.prediction) as entries, \\\n    int(c.prediction) as exits \\\n  from \\\n  df_desafio_num a \\\n  inner join lr_predictions_entries b \\\n  on a.id = b.id \\\n  inner join lr_predictions_exits c \\\n  on a.id = c.id\"\n)\n\ndf_2016.show(10)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Gráfico de barras\ndisplay(df_2016.groupBy(  \n  'dt_month'  \n).agg(\n  sum('entries'),\n  sum('exits')\n).orderBy('dt_month')\n)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["#### Modelo de Deep Learning - Elephas"],"metadata":{}},{"cell_type":"code","source":["# Etapa 01 - Transformações: VectorAssembler\n\n# Variáveis para modelo\nfeatures = ['entries', 'exits', 'dt_year', 'dt_month', 'dt_day', 'dt_dayofy', 'dt_hour', 'dt_min', 'dt_week_no', 'dt_int', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx', 'linename_idx', 'division_idx', 'desc_idx']\nvar_predict = 'entries'\nfeatures.remove(var_predict)\n\n# Vetorização de features\nvectorAssembler = VectorAssembler(inputCols = features, outputCol = 'features')\ndf_desafio = vectorAssembler.transform(df_desafio_num)\n\n# Seleciona features + interesse (y)\ndf_desafio = df_desafio.select(['features', var_predict])\n\n# Divisão de dados para treino e teste\nsplits = df_desafio.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Etapa 02 - Transformações: StringIndexer & StandardScaler\n\nstring_indexer = StringIndexer(inputCol=var_predict, outputCol=\"index_category\")\nfitted_indexer = string_indexer.fit(train_df)\nindexed_df = fitted_indexer.transform(train_df)\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\nfitted_scaler = scaler.fit(indexed_df)\nscaled_df = fitted_scaler.transform(indexed_df)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Modelo Keras Deep Learning\nnb_classes = train_df.select(var_predict).distinct().count()\ninput_dim = len(train_df.select(\"features\").first()[0])\n\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(input_dim,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Define o otimizador para o Elephas (Adadelta)\nAdadelta = elephas_optimizers.Adadelta()\n\n# Cria o SparkML Estimator e configurar parâmetros\nestimator = ElephasEstimator()\nestimator.setFeaturesCol(\"scaled_features\")             \nestimator.setLabelCol(\"index_category\")                \nestimator.set_keras_model_config(model.to_yaml())       \nestimator.set_optimizer_config(Adadelta.get_config())   # < Bug aqui #20\nestimator.set_categorical_labels(True)\nestimator.set_nb_classes(nb_classes)\nestimator.set_num_workers(2) \nestimator.set_epochs(20) \nestimator.set_batch_size(128)\nestimator.set_verbosity(1)\nestimator.set_validation_split(0.15)\nestimator.set_loss('squaredError')"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Cria SparkML Pipelines\npipeline = Pipeline(stages=[string_indexer, scaler, estimator])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# Bug encontrado durante o processo de fit, tentando resolver: https://github.com/maxpumperla/elephas/issues/122\n# Fitting do modelo\nfitted_pipeline = pipeline.fit(train_df) # Fit model to data"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# Predição para train/teste\nprediction = fitted_pipeline.transform(train_df)\n# prediction = fitted_pipeline.transform(test_df)\npnl = prediction.select(\"index_category\", \"prediction\")\npnl.show(10)\n\n# Avaliação de resultados\nprediction_and_label = pnl.map(lambda row: (row.index_category, row.prediction))\nmetrics = MulticlassMetrics(prediction_and_label)\nprint(metrics.precision())"],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"poc-ny-subway","notebookId":1932292055738400},"nbformat":4,"nbformat_minor":0}
