{"cells":[{"cell_type":"code","source":["from pyspark import *\n#from pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["# Exploratory data analysis (EDA)"],"metadata":{}},{"cell_type":"code","source":["# Carrega dataset do desafio\n#display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))\ndf_desafio = spark.read.format(\"csv\").options(header='true').load(\"/FileStore/tables/*.csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Elimina na's e duplicados do df\ndf_desafio_v2 = df_desafio.dropna(how='any').dropDuplicates()\n\n# Ajusta tipo de colunas\ndf_desafio_v2 = df_desafio.selectExpr(\n  'cast(time as timestamp) time',\n  'ca',\n  'unit',\n  'scp',\n  'station',\n  'linename',\n  'division',\n  'desc',\n  'cast(entries as int) entries',\n  'cast(exits as int) exits'\n)\n\n# Features para visão temporal\ndf_desafio_v2 = df_desafio_v2.withColumn(\n  \"dt_year\",\n  year(col(\"time\"))\n).withColumn(\n  \"dt_month\",\n  month(col(\"time\"))\n).withColumn(\n  \"dt_day\",\n  dayofmonth(col(\"time\"))\n).withColumn(\n  \"dt_dayofy\",\n  dayofyear(col(\"time\"))\n).withColumn(\n  \"dt_hour\",\n  hour(col(\"time\"))\n).withColumn(\n  \"dt_min\",\n  minute(col(\"time\"))\n).withColumn(\n  \"dt_week_no\",\n  weekofyear(col(\"time\"))\n).withColumn(\n  \"dt_int\",\n  unix_timestamp(col(\"time\"))\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Check: 79609191 / 79130015 / 79045675\ncount_desafio = df_desafio.count()\ncount_desafio_na = df_desafio.dropna(how='any').count()\ncount_desafio_final = df_desafio.dropna(how='any').dropDuplicates().count()\n\ndf_amostras = sc.parallelize([\n  ('antes',count_desafio,0,0,0),\n  ('depois',0,count_desafio_final,count_desafio-count_desafio_na,count_desafio_na-count_desafio_final)\n]).toDF(['AMOSTRAS','TOTAL','UNICO','NA','DUPLICADO'])\n\ndisplay(df_amostras)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df_amostras_v2 = sc.parallelize([\n  ('',count_desafio-count_desafio_na,count_desafio_na-count_desafio_final)\n]).toDF(['AMOSTRAS','NA','DUPLICADO'])\n\ndisplay(df_amostras_v2)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Método para dummys\npivot_cols = ['ca', 'unit', 'scp', 'station', 'linename', 'division', 'desc']\nkeys = df_desafio_v2.columns\nbefore = df_desafio_v2\n\ndef join_all(dfs,keys):\n    if len(dfs) > 1:\n        return dfs[0].join(join_all(dfs[1:],keys), on = keys, how = 'inner')\n    else:\n        return dfs[0]\n\ndfs = []\ncombined = []\nfor pivot_col in pivot_cols:\n    pivotDF = before.groupBy(keys).pivot(pivot_col).count()\n    new_names = pivotDF.columns[:len(keys)] +  [\"e_{0}_{1}\".format(pivot_col, c) for c in pivotDF.columns[len(keys):]]        \n    df = pivotDF.toDF(*new_names).fillna(0)    \n    combined.append(df)\n\ndf_desafio_v3 = join_all(combined,keys)\n\n#contagem = df_desafio_v3.columns\n#len(contagem) ==> 2286 features"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#-7254\n#Out[26]: 79052929\n\ndf_desafio_v3.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#display(df_desafio_v2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#display(df_desafio_v2)"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"poc-ny-subway","notebookId":1932292055738400},"nbformat":4,"nbformat_minor":0}
