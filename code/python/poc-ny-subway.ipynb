{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\nfrom pyspark import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import StringIndexer, Bucketizer, VectorAssembler, StandardScaler\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.mllib.stat import Statistics\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.utils import np_utils, generic_utils\nfrom elephas.ml_model import ElephasEstimator\nfrom elephas import optimizers as elephas_optimizers"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\npdl4j: required uberjar not found, building with docker...\n/databricks/python/lib/python3.5/site-packages/pydl4j/pydl4j.py:272: UserWarning: Docker unavailable. Attempting alternate implementation.\n  warnings.warn(&quot;Docker unavailable. Attempting alternate implementation.&quot;)\nWARNING\n</div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["#### Exploratory data analysis (EDA)"],"metadata":{}},{"cell_type":"code","source":["# Carrega dataset do desafio (basta descompactar todos na mesma pasta)\n#display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))\ndf_desafio = spark.read.format(\"csv\").options(header='true').load(\"/FileStore/tables/*.csv\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Elimina na's e duplicados do df\ndf_desafio_v2 = df_desafio.dropna(how='any').dropDuplicates()\n\n# Ajusta tipo de colunas\ndf_desafio_v2 = df_desafio_v2.selectExpr(\n  'cast(time as timestamp) time',\n  'ca',\n  'unit',\n  'scp',\n  'station',\n  'linename',\n  'division',\n  'desc',\n  'cast(entries as int) entries',\n  'cast(exits as int) exits'\n)\n\n# Features para visão temporal\ndf_desafio_v2 = df_desafio_v2.withColumn(\n  \"dt_year\",\n  year(col(\"time\"))\n).withColumn(\n  \"dt_month\",\n  month(col(\"time\"))\n).withColumn(\n  \"dt_day\",\n  dayofmonth(col(\"time\"))\n).withColumn(\n  \"dt_dayofy\",\n  dayofyear(col(\"time\"))  \n).withColumn(\n  \"dt_hour\",\n  hour(col(\"time\"))\n).withColumn(\n  \"dt_min\",\n  minute(col(\"time\"))\n).withColumn(\n  \"dt_week_no\",\n  weekofyear(col(\"time\"))\n).withColumn(\n  \"dt_int\",\n  unix_timestamp(col(\"time\"))\n).withColumn(\n  \"dt_month_year\",\n  date_format(col(\"time\"), \"Y-MM\")\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Intervalos para buckets\nsplits = [-30000000, -20000000, -10000000, 0.0, 10000000, 20000000, 30000000]\n\n# ===> Bucket: 'entries'\n# dataFrame = df_desafio_v2.select(col('entries'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"entries\", outputCol=\"bucketedFeatures\")\n# bucketedData_entries = bucketizer.transform(dataFrame)\n# sorted(bucketedData_entries.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=15721),\n#  Row(bucketedFeatures=1.0, count=4028),\n#  Row(bucketedFeatures=2.0, count=496),\n#  Row(bucketedFeatures=4.0, count=71457425), ==> Begin\n#  Row(bucketedFeatures=5.0, count=5052549),  <== End\n#  Row(bucketedFeatures=6.0, count=278162),\n#  Row(bucketedFeatures=7.0, count=2237294)]\n\n# ===> Bucket: 'exits'\n# dataFrame = df_desafio_v2.select(col('exits'))\n# bucketizer = Bucketizer(splits=splits, inputCol=\"exits\", outputCol=\"bucketedFeatures\")\n# bucketedData_exits = bucketizer.transform(dataFrame)\n# sorted(bucketedData_exits.groupBy(\"bucketedFeatures\").count().collect())\n# [Row(bucketedFeatures=0.0, count=30576),\n#  Row(bucketedFeatures=2.0, count=576),\n#  Row(bucketedFeatures=3.0, count=1),\n#  Row(bucketedFeatures=4.0, count=73751241), ==> Begin\n#  Row(bucketedFeatures=5.0, count=3698911),  <== End\n#  Row(bucketedFeatures=6.0, count=413740),\n#  Row(bucketedFeatures=7.0, count=1150630)]\n\n# Parâmetros para filtros de outliers\noutlier_begin = 0\noutlier_end = 20000000\n\ndf_desafio_v2 = df_desafio_v2.where((col('entries')>=outlier_begin) & (col('entries')<=outlier_end) & (col('exits')>=outlier_begin) & (col('exits')<=outlier_end))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Check: 79.609.191 / 79.130.015 / 79.045.675 / 75.923.980\ncount_desafio = df_desafio.count()\ncount_desafio_na = df_desafio.dropna(how='any').count()\ncount_desafio_final = df_desafio.dropna(how='any').dropDuplicates().count()\ncount_desafio_outliers = df_desafio_v2.count()\n\ndf_amostras = sc.parallelize([\n  ('antes',count_desafio,0,0,0,0),\n  ('depois',0,count_desafio_final,count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','TOTAL','UNICO','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df_amostras_v2 = sc.parallelize([\n  ('',count_desafio-count_desafio_na,count_desafio_na-count_desafio_final,count_desafio_final-count_desafio_outliers)\n]).toDF(['AMOSTRAS','NA','DUPLICADO','OUTLIERS'])\n\ndisplay(df_amostras_v2)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Método para variáveis categóricas (dummys) ~14.59 minutes\nlista_idx = ['ca', 'unit', 'scp', 'station', 'linename', 'division', 'desc']\nindexers = [StringIndexer(inputCol=column,outputCol=column+\"_idx\").fit(df_desafio_v2) for column in lista_idx]\npipeline = Pipeline(stages=indexers)\ndf_desafio_v2 = pipeline.fit(df_desafio_v2).transform(df_desafio_v2)\n#display(df_desafio_v2)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df_desafio_v2.write.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio_num.write.parquet(\"/FileStore/tables/df_desafio_ml.parquet\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-189668614051512&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansired\">#df_desafio_v2.write.parquet(&quot;/FileStore/tables/df_desafio.parquet&quot;)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>df_desafio_num<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/FileStore/tables/df_desafio_ml.parquet&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;df_desafio_num&apos; is not defined</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#df_graficos = df_desafio_v2.filter(col('dt_year')=='2017')\ndf_graficos = df_desafio_v2.groupBy(\n  'dt_month_year',\n  'dt_year',\n  'dt_month',\n  'dt_day',\n  'dt_hour'\n).agg(\n  sum('entries'),\n  sum('exits')\n).orderBy(\n  \"dt_month_year\"\n).toPandas()\n\n#df_desafio_num.describe().toPandas().transpose()\n#df_desafio_num.dtypes\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Carga de Parquet para modelos"],"metadata":{}},{"cell_type":"code","source":["# Carrega Parquet\ndf_desafio_pqt = spark.read.parquet(\"/FileStore/tables/df_desafio.parquet\")\ndf_desafio_num = df_desafio_pqt.select(\n  'entries','exits','dt_year','dt_month','dt_day','dt_dayofy',\n  'dt_hour','dt_min','dt_week_no','dt_int','ca_idx','unit_idx',\n  'scp_idx','station_idx','linename_idx','division_idx','desc_idx'\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["#### Correlação entre features"],"metadata":{}},{"cell_type":"code","source":["col_names = df_desafio.columns\nfeatures = df_desafio.rdd.map(lambda row: row[0:])\ncorr_mat = Statistics.corr(features, method=\"pearson\")\ncorr_df = pd.DataFrame(corr_mat)\ncorr_df.index, corr_df.columns = col_names, col_names\nspark.createDataFrame(corr_df).write.parquet(\"/FileStore/tables/corr_df.parquet\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### Modelo de Machine Learning - Regressão Linear"],"metadata":{}},{"cell_type":"code","source":["# Verorização de features\ncolunas = ['exits', 'dt_year', 'dt_month', 'dt_day', 'dt_dayofy', 'dt_hour', 'dt_min', 'dt_week_no', 'dt_int', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx', 'linename_idx', 'division_idx', 'desc_idx']\nvectorAssembler = VectorAssembler(inputCols = colunas, outputCol = 'features')\nvdf_desafio_num = vectorAssembler.transform(df_desafio_num)\nvdf_desafio_num.take(1)\n\n# Seleciona features de interesse\nvdf_desafio_num = vdf_desafio_num.select(['features', 'entries'])\nvdf_desafio_num.show(3)\n\n# Split para dados de treino/teste\nsplits = vdf_desafio_num.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Treinando os dados de treino para predizer os dados de teste\nlr = LinearRegression(featuresCol = 'features', labelCol='entries', maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlr_model = lr.fit(train_df)\n\n#Salva modelo treinado\nlr_model.save(\"/FileStore/tables/lr_model\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#Resumo do treinamento\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"Intercept: \" + str(lr_model.intercept))\n\n# Resumo do modelo treinado\ntrainingSummary = lr_model.summary\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2: %f\" % trainingSummary.r2)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Descritiva da variável de interesse treinada\ntrain_df.describe().show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Predição dos dados de teste vs Real\nlr_predictions = lr_model.transform(test_df)\nlr_predictions.select(\"prediction\",\"entries\",\"features\").show(5)\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"entries\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n\n# Resultado RMSE do modelo de teste\ntest_result = lr_model.evaluate(test_df)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Histograma para residuals\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n#display(trainingSummary.residuals)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#### Modelo de Deep Learning - Elephas"],"metadata":{}},{"cell_type":"code","source":["# Etapa 01 - Transformações: VectorAssembler\n\n# Variáveis para modelo\nfeatures = ['entries', 'exits', 'dt_year', 'dt_month', 'dt_day', 'dt_dayofy', 'dt_hour', 'dt_min', 'dt_week_no', 'dt_int', 'ca_idx', 'unit_idx', 'scp_idx', 'station_idx', 'linename_idx', 'division_idx', 'desc_idx']\nvar_predict = 'entries'\nfeatures.remove(var_predict)\n\n# Vetorização de features\nvectorAssembler = VectorAssembler(inputCols = features, outputCol = 'features')\ndf_desafio = vectorAssembler.transform(df_desafio_num)\n\n# Seleciona features + interesse (y)\ndf_desafio = df_desafio.select(['features', var_predict])\n\n# Divisão de dados para treino e teste\nsplits = df_desafio.randomSplit([0.7, 0.3])\ntrain_df = splits[0]\ntest_df = splits[1]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["# Etapa 02 - Transformações: StringIndexer & StandardScaler\nstring_indexer = StringIndexer(inputCol=var_predict, outputCol=\"index_category\")\nfitted_indexer = string_indexer.fit(train_df)\nindexed_df = fitted_indexer.transform(train_df)\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\nfitted_scaler = scaler.fit(indexed_df)\nscaled_df = fitted_scaler.transform(indexed_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["# Keras Deep Learning model\nnb_classes = train_df.select(var_predict).distinct().count()\ninput_dim = len(train_df.select(\"features\").first()[0])\n\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(input_dim,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Define elephas optimizer (which tells the model how to aggregate updates on the Spark master)\nAdadelta = elephas_optimizers.Adadelta()\n\n# Initialize SparkML Estimator and set all relevant properties\nestimator = ElephasEstimator()\nestimator.setFeaturesCol(\"scaled_features\")             # These two come directly from pyspark,\nestimator.setLabelCol(\"index_category\")                 # hence the camel case. Sorry :)\nestimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\nestimator.set_optimizer_config(Adadelta.get_config())   # Provide serialized Elephas optimizer\nestimator.set_categorical_labels(True)\nestimator.set_nb_classes(nb_classes)\nestimator.set_num_workers(3)  # We just use one worker here. Feel free to adapt it.\nestimator.set_epochs(20) \nestimator.set_batch_size(128)\nestimator.set_verbosity(1)\nestimator.set_validation_split(0.15)\nestimator.set_loss('squaredError')"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# SparkML Pipelines\npipeline = Pipeline(stages=[string_indexer, scaler, estimator])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Fitting and evaluating the pipeline\nfitted_pipeline = pipeline.fit(train_df) # Fit model to data"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["prediction = fitted_pipeline.transform(train_df) # Evaluate on train data.\n# prediction = fitted_pipeline.transform(test_df) # <-- The same code evaluates test data.\npnl = prediction.select(\"index_category\", \"prediction\")\npnl.show(100)\n\nprediction_and_label = pnl.map(lambda row: (row.index_category, row.prediction))\nmetrics = MulticlassMetrics(prediction_and_label)\nprint(metrics.precision())"],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"poc-ny-subway","notebookId":1932292055738400},"nbformat":4,"nbformat_minor":0}
